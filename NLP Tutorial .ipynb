{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "I start off with explaning few basic NLP tasks and Regular Expressions (RegEx) and then move onto writing structed programs like sentiment analysis and other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "# from NLTK's book module, load all items.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "text7\n",
    "# A concordance view shows us every occurrence of a given word, together with some context.\n",
    "text1.concordance(\"monstrous\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true contemptible christian abundant few part mean careful puzzled\n",
      "mystifying passing curious loving wise doleful gamesome singular\n",
      "delightfully perilous fearless\n"
     ]
    }
   ],
   "source": [
    "sent7\n",
    "# other words appear in a similar range of context can be searched \n",
    "# by appending the term similar to the name of the text in question\n",
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_pretty am_glad a_lucky is_pretty be_glad\n"
     ]
    }
   ],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15bf00540f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dispersion Plot: Each stripe represents an instance of a word, and each row represents the entire text.\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\",'liberty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7)\n",
    "#  A token is the technical name for a sequence of characters. Thus, text 7 has 100676 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wotteth',\n",
       " 'labour',\n",
       " 'young',\n",
       " 'Naphtuhim',\n",
       " 'pleasure',\n",
       " 'business',\n",
       " 'protest',\n",
       " 'ravin',\n",
       " 'pleasant',\n",
       " 'Elah',\n",
       " 'gave',\n",
       " 'wells',\n",
       " 'An',\n",
       " 'Happy',\n",
       " 'lie',\n",
       " 'ascending',\n",
       " 'appe',\n",
       " 'cities',\n",
       " 'commune',\n",
       " 'Give',\n",
       " 'eatest',\n",
       " 'Asher',\n",
       " 'prophet',\n",
       " 'rul',\n",
       " 'Bilhah',\n",
       " 'mou',\n",
       " 'wounding',\n",
       " 'troubled',\n",
       " 'Uzal',\n",
       " 'fountain',\n",
       " 'bury',\n",
       " 'shew',\n",
       " 'changes',\n",
       " 'born',\n",
       " 'strove',\n",
       " 'Jezer',\n",
       " 'clo',\n",
       " 'communing',\n",
       " 'portion',\n",
       " 'Akan',\n",
       " 'Moab',\n",
       " 'received',\n",
       " 'trees',\n",
       " 'pluckt',\n",
       " 'cause',\n",
       " 'Take',\n",
       " 'child',\n",
       " 'heifer',\n",
       " 'Muppim',\n",
       " 'haste',\n",
       " 'appear',\n",
       " 'mischief',\n",
       " 'tr',\n",
       " 'establish',\n",
       " 'buryingplace',\n",
       " 'proved',\n",
       " 'longedst',\n",
       " 'hide',\n",
       " 'finished',\n",
       " 'saying',\n",
       " 'Havilah',\n",
       " 'savour',\n",
       " 'lands',\n",
       " 'drought',\n",
       " 'sadly',\n",
       " 'yielded',\n",
       " 'Zebulun',\n",
       " 'known',\n",
       " 'eyed',\n",
       " 'Bethlehem',\n",
       " 'Haste',\n",
       " 'Sinite',\n",
       " 'Drink',\n",
       " 'Sojourn',\n",
       " 'Thou',\n",
       " 'Serug',\n",
       " 'freely',\n",
       " 'anger',\n",
       " 'Male',\n",
       " 'thither',\n",
       " 'elders',\n",
       " 'archer',\n",
       " 'little',\n",
       " 'trade',\n",
       " 'spent',\n",
       " 'run',\n",
       " 'tower',\n",
       " 'Hebron',\n",
       " 'Almodad',\n",
       " 'How',\n",
       " 'Shelah',\n",
       " 'then',\n",
       " 'office',\n",
       " 'co',\n",
       " 'Hamul',\n",
       " 'Jabbok',\n",
       " 'yea',\n",
       " 'before',\n",
       " 'your',\n",
       " 'peaceably',\n",
       " 'loss',\n",
       " 'for',\n",
       " 'households',\n",
       " 'To',\n",
       " 'Cause',\n",
       " 'visions',\n",
       " 'grew',\n",
       " 'Erech',\n",
       " 'feebler',\n",
       " 'goods',\n",
       " 'loud',\n",
       " 'beget',\n",
       " 'twenty',\n",
       " 'head',\n",
       " 'la',\n",
       " 'abide',\n",
       " 'Philistim',\n",
       " 'hills',\n",
       " 'halted',\n",
       " 'Eliphaz',\n",
       " 'have',\n",
       " 'Zibeon',\n",
       " 'an',\n",
       " 'overcome',\n",
       " 'dark',\n",
       " 'poured',\n",
       " 'unleavened',\n",
       " 'welfare',\n",
       " 'Jegarsahadutha',\n",
       " 'dignity',\n",
       " 'Zillah',\n",
       " 'mine',\n",
       " 'stronger',\n",
       " 'drunken',\n",
       " 'decreased',\n",
       " 'Sabtech',\n",
       " 'dew',\n",
       " 'messenger',\n",
       " 'Aholibamah',\n",
       " 'arose',\n",
       " 'Girgashites',\n",
       " 'binding',\n",
       " 'brimstone',\n",
       " 'deceived',\n",
       " 'Midian',\n",
       " 'Cana',\n",
       " 'bank',\n",
       " 'pleaseth',\n",
       " 'Thorns',\n",
       " 'Zohar',\n",
       " 'afar',\n",
       " 'doubt',\n",
       " 'Jobab',\n",
       " 'returned',\n",
       " 'bre',\n",
       " 'presented',\n",
       " 'repented',\n",
       " 'denied',\n",
       " 'multiply',\n",
       " 'famine',\n",
       " 'Euphrat',\n",
       " 'greater',\n",
       " 'tak',\n",
       " 'Aner',\n",
       " 'They',\n",
       " 'servant',\n",
       " 'raise',\n",
       " 'named',\n",
       " 'wearied',\n",
       " 'lingered',\n",
       " 'upon',\n",
       " 'drinking',\n",
       " 'leap',\n",
       " 'tents',\n",
       " 'Egyptian',\n",
       " 'Alvan',\n",
       " 'Succoth',\n",
       " 'riches',\n",
       " 'comfort',\n",
       " 'butter',\n",
       " 'nor',\n",
       " 'Huz',\n",
       " 'Judah',\n",
       " 'Dumah',\n",
       " 'upright',\n",
       " 'hairs',\n",
       " 'Ask',\n",
       " 'Judith',\n",
       " 'fury',\n",
       " 'nine',\n",
       " 'daughers',\n",
       " 'walketh',\n",
       " 'Zidon',\n",
       " 'Gatam',\n",
       " 'even',\n",
       " 'Canaanitish',\n",
       " 'man',\n",
       " 'pray',\n",
       " 'Horite',\n",
       " 'Salah',\n",
       " 'past',\n",
       " 'she',\n",
       " 'reason',\n",
       " 'beari',\n",
       " 'seedtime',\n",
       " 'ripe',\n",
       " 'Babel',\n",
       " 'prince',\n",
       " 'Mamre',\n",
       " 'rolled',\n",
       " 'Bered',\n",
       " 'fearest',\n",
       " 'country',\n",
       " 'moving',\n",
       " 'han',\n",
       " 'kine',\n",
       " 'blameless',\n",
       " 'prayed',\n",
       " 'healed',\n",
       " 'rain',\n",
       " 'selfsame',\n",
       " 'certain',\n",
       " 'caused',\n",
       " 'imagination',\n",
       " 'Lot',\n",
       " 'eaten',\n",
       " 'Heber',\n",
       " 'lack',\n",
       " 'rider',\n",
       " 'northward',\n",
       " 'priest',\n",
       " 'Hai',\n",
       " 'grap',\n",
       " 'him',\n",
       " 'smell',\n",
       " 'blessing',\n",
       " 'mourning',\n",
       " 'mules',\n",
       " 'hire',\n",
       " 'her',\n",
       " 'venison',\n",
       " 'fai',\n",
       " 'fly',\n",
       " 'gat',\n",
       " 'day',\n",
       " 'dwe',\n",
       " 'word',\n",
       " 'dale',\n",
       " 'When',\n",
       " 'O',\n",
       " 'kindness',\n",
       " 'Pharez',\n",
       " 'inheritance',\n",
       " 'cursed',\n",
       " 'hil',\n",
       " 'spilled',\n",
       " 'droves',\n",
       " 'marriages',\n",
       " 'herd',\n",
       " 'age',\n",
       " 'hundredth',\n",
       " 'Jetheth',\n",
       " 'grisled',\n",
       " 'perceived',\n",
       " 'faults',\n",
       " 'evil',\n",
       " 'All',\n",
       " 'placed',\n",
       " 'creepeth',\n",
       " 'gifts',\n",
       " 'ne',\n",
       " 'salt',\n",
       " 'none',\n",
       " 'Perizzite',\n",
       " 'whose',\n",
       " 'if',\n",
       " 'Nod',\n",
       " 'daught',\n",
       " 'tenor',\n",
       " 'Of',\n",
       " 'tent',\n",
       " 'God',\n",
       " 'charge',\n",
       " 'manner',\n",
       " 'beneath',\n",
       " 'Terah',\n",
       " 'Nebajoth',\n",
       " 'Ephah',\n",
       " 'dipped',\n",
       " 'birthday',\n",
       " 'Ehi',\n",
       " 'heed',\n",
       " 'sister',\n",
       " 'whereon',\n",
       " 'love',\n",
       " 'Nimrod',\n",
       " 'Get',\n",
       " 'ask',\n",
       " 'seven',\n",
       " 'Cush',\n",
       " 'chode',\n",
       " 'season',\n",
       " 'Eno',\n",
       " 'basket',\n",
       " 'turtledove',\n",
       " 'horse',\n",
       " 'Kedemah',\n",
       " 'stars',\n",
       " 'behold',\n",
       " 'circumcised',\n",
       " 'doe',\n",
       " 'shortly',\n",
       " 'Shur',\n",
       " 'Leah',\n",
       " 'Mahanaim',\n",
       " 'bow',\n",
       " 'rebuked',\n",
       " 'least',\n",
       " 'meeteth',\n",
       " 'purchase',\n",
       " 'daughter',\n",
       " 'years',\n",
       " 'laid',\n",
       " 'visit',\n",
       " 'Ziphion',\n",
       " 'Fifteen',\n",
       " 'Hear',\n",
       " 'mention',\n",
       " 'increased',\n",
       " 'birds',\n",
       " 'posterity',\n",
       " 'Jerah',\n",
       " 'Adah',\n",
       " 'Only',\n",
       " 'circumcise',\n",
       " 'charged',\n",
       " 'poor',\n",
       " 'conceive',\n",
       " 'excel',\n",
       " 'took',\n",
       " 'espied',\n",
       " 'good',\n",
       " 'earth',\n",
       " 'mayest',\n",
       " 'Feed',\n",
       " 'eat',\n",
       " 'ungirded',\n",
       " 'interpreted',\n",
       " 'kneel',\n",
       " 'Areli',\n",
       " 'fetched',\n",
       " 'solemnly',\n",
       " 'saddled',\n",
       " 'forgat',\n",
       " 'seeing',\n",
       " 'flo',\n",
       " 'escape',\n",
       " 'compassed',\n",
       " 'himself',\n",
       " 'husbandman',\n",
       " 'Out',\n",
       " 'told',\n",
       " 'way',\n",
       " 'whole',\n",
       " 'seed',\n",
       " 'continued',\n",
       " 'Except',\n",
       " 'forth',\n",
       " 'sight',\n",
       " 'despised',\n",
       " 'looked',\n",
       " 'widowhood',\n",
       " 'be',\n",
       " 'learned',\n",
       " 'asses',\n",
       " 'down',\n",
       " 'wolf',\n",
       " 'sacks',\n",
       " 'themselv',\n",
       " 'Abel',\n",
       " 'His',\n",
       " 'his',\n",
       " 'gard',\n",
       " 'stood',\n",
       " 'Reuel',\n",
       " 'Two',\n",
       " ';)',\n",
       " 'feeding',\n",
       " 'Jemuel',\n",
       " 'chi',\n",
       " 'offended',\n",
       " 'On',\n",
       " 'he',\n",
       " 'pilled',\n",
       " 'Salem',\n",
       " 'stone',\n",
       " 'bowed',\n",
       " 'Enos',\n",
       " 'fill',\n",
       " 'rose',\n",
       " 'Raamah',\n",
       " 'Speak',\n",
       " 'Mibzar',\n",
       " 'children',\n",
       " 'Whence',\n",
       " 'names',\n",
       " 'Both',\n",
       " 'deliverance',\n",
       " 'destroyed',\n",
       " 'amongst',\n",
       " 'shoelatchet',\n",
       " 'cry',\n",
       " 'Korah',\n",
       " 'indeed',\n",
       " 'garments',\n",
       " 'void',\n",
       " 'east',\n",
       " 'obeisance',\n",
       " 'guiding',\n",
       " 'rooms',\n",
       " 'cubit',\n",
       " 'henceforth',\n",
       " 'barren',\n",
       " 'Cursed',\n",
       " 'Thirty',\n",
       " 'Gera',\n",
       " 'those',\n",
       " 'cubits',\n",
       " 'Ishbak',\n",
       " 'Jordan',\n",
       " 'in',\n",
       " 'descending',\n",
       " 'begin',\n",
       " 'eyes',\n",
       " 'boug',\n",
       " 'He',\n",
       " 'habitations',\n",
       " 'excellency',\n",
       " 'belong',\n",
       " 'five',\n",
       " 'giants',\n",
       " 'Moreh',\n",
       " 'thee',\n",
       " 'reserved',\n",
       " 'Isa',\n",
       " 'Horites',\n",
       " 'else',\n",
       " 'breasts',\n",
       " 'Discern',\n",
       " 'fa',\n",
       " 'answer',\n",
       " 'slimepits',\n",
       " 'breaking',\n",
       " 'stones',\n",
       " 'kn',\n",
       " 'tabret',\n",
       " 'built',\n",
       " 'onyx',\n",
       " 'burning',\n",
       " 'meanest',\n",
       " 'people',\n",
       " 'Unto',\n",
       " 'Zuzims',\n",
       " 'bones',\n",
       " 'surety',\n",
       " 'serva',\n",
       " 'generations',\n",
       " 'reward',\n",
       " 'Shammah',\n",
       " 'younger',\n",
       " 'springing',\n",
       " 'pasture',\n",
       " 'Make',\n",
       " 'buried',\n",
       " 'Judge',\n",
       " 'sweat',\n",
       " 'Not',\n",
       " 'elder',\n",
       " 'subtil',\n",
       " 'furnace',\n",
       " 'slaughter',\n",
       " 'slew',\n",
       " 'edge',\n",
       " 'Kohath',\n",
       " 'fourteenth',\n",
       " 'refused',\n",
       " 'Merari',\n",
       " 'hunter',\n",
       " 'being',\n",
       " 'But',\n",
       " 'Samlah',\n",
       " 'magicians',\n",
       " 'sou',\n",
       " 'wrestlings',\n",
       " 'Reub',\n",
       " 'Padan',\n",
       " 'am',\n",
       " 'money',\n",
       " 'blesseth',\n",
       " 'Iram',\n",
       " 'offering',\n",
       " 'plagued',\n",
       " 'Shinab',\n",
       " 'Togarmah',\n",
       " 'except',\n",
       " 'belly',\n",
       " 'Moriah',\n",
       " 'prepared',\n",
       " 'dealt',\n",
       " 'followed',\n",
       " 'Simeon',\n",
       " 'under',\n",
       " 'Hast',\n",
       " 'Abr',\n",
       " 'shed',\n",
       " 'ten',\n",
       " 'foremost',\n",
       " 'hastened',\n",
       " 'desire',\n",
       " 'lord',\n",
       " 'themselves',\n",
       " 'rams',\n",
       " 'scarlet',\n",
       " 'handmaids',\n",
       " 'Obal',\n",
       " 'Ur',\n",
       " 'Shel',\n",
       " 'floor',\n",
       " 'Shebah',\n",
       " 'displease',\n",
       " 'door',\n",
       " 'rent',\n",
       " 'Gether',\n",
       " 'ea',\n",
       " 'think',\n",
       " 'covering',\n",
       " 'blessings',\n",
       " 'month',\n",
       " 'plant',\n",
       " 'men',\n",
       " 'food',\n",
       " 'oath',\n",
       " 'Rosh',\n",
       " 'Heaven',\n",
       " 'firstborn',\n",
       " 'Leummim',\n",
       " 'wild',\n",
       " 'caught',\n",
       " 'Laban',\n",
       " 'cru',\n",
       " 'eighty',\n",
       " 'destroy',\n",
       " 'judgment',\n",
       " 'thousand',\n",
       " 'choice',\n",
       " 'fail',\n",
       " 'behind',\n",
       " 'LO',\n",
       " '.)',\n",
       " 'measures',\n",
       " 'light',\n",
       " 'Lehabim',\n",
       " 'linen',\n",
       " 'Elbethel',\n",
       " 'prosperous',\n",
       " 'Husham',\n",
       " 'reach',\n",
       " 'togeth',\n",
       " 'truly',\n",
       " 'jud',\n",
       " 'Shem',\n",
       " 'Go',\n",
       " 'about',\n",
       " 'commandment',\n",
       " 'With',\n",
       " 'gavest',\n",
       " 'imagined',\n",
       " 'hor',\n",
       " 'cometh',\n",
       " 'afterwards',\n",
       " 'comi',\n",
       " 'messengers',\n",
       " 'sto',\n",
       " 'obey',\n",
       " 'chief',\n",
       " 'Milcah',\n",
       " 'knowest',\n",
       " 'Should',\n",
       " 'Euphrates',\n",
       " 'bake',\n",
       " 'angry',\n",
       " 'two',\n",
       " 'fallen',\n",
       " 'overtake',\n",
       " 'handmaid',\n",
       " 'gathered',\n",
       " 'olive',\n",
       " 'able',\n",
       " 'lawgiver',\n",
       " 'cool',\n",
       " 'brother',\n",
       " 'heaven',\n",
       " 'kill',\n",
       " 'Abram',\n",
       " 'wash',\n",
       " 'appointed',\n",
       " 'ruler',\n",
       " 'drink',\n",
       " 'seventy',\n",
       " 'feet',\n",
       " 'bless',\n",
       " 'quiver',\n",
       " 'sle',\n",
       " 'heads',\n",
       " 'troop',\n",
       " 'experience',\n",
       " 'kid',\n",
       " 'displeased',\n",
       " 'vagabond',\n",
       " 'very',\n",
       " 'lieth',\n",
       " 'unto',\n",
       " 'therefore',\n",
       " 'old',\n",
       " 'besides',\n",
       " 'rained',\n",
       " 'beasts',\n",
       " 'speed',\n",
       " 'regard',\n",
       " 'blessed',\n",
       " 'having',\n",
       " 'straw',\n",
       " 'must',\n",
       " '!',\n",
       " 'height',\n",
       " 'so',\n",
       " 'Resen',\n",
       " 'mocked',\n",
       " 'wo',\n",
       " 'peaceable',\n",
       " 'kinds',\n",
       " 'erected',\n",
       " 'whereof',\n",
       " 'out',\n",
       " 'Tidal',\n",
       " 'Ephra',\n",
       " 'talking',\n",
       " 'altogether',\n",
       " 'itself',\n",
       " 'cattle',\n",
       " 'rode',\n",
       " '?',\n",
       " 'Deliver',\n",
       " 'merciful',\n",
       " 'who',\n",
       " 'sporting',\n",
       " 'Kadmonites',\n",
       " 'full',\n",
       " 'victuals',\n",
       " 'iniquity',\n",
       " 'weig',\n",
       " 'yearn',\n",
       " 'guiltiness',\n",
       " 'shore',\n",
       " 'Perizzit',\n",
       " 'womenservan',\n",
       " 'boldly',\n",
       " 'Art',\n",
       " 'drove',\n",
       " 'Pau',\n",
       " 'Hemdan',\n",
       " 'withhold',\n",
       " 'Shiloh',\n",
       " 'tops',\n",
       " 'Ararat',\n",
       " 'winter',\n",
       " 'Zeboim',\n",
       " 'savoury',\n",
       " 'possessions',\n",
       " 'praise',\n",
       " 'milch',\n",
       " 'plenty',\n",
       " 'Benjamin',\n",
       " 'armed',\n",
       " 'weep',\n",
       " 'Bela',\n",
       " 'endued',\n",
       " 'Phuvah',\n",
       " 'worshipped',\n",
       " 'enquire',\n",
       " 'thine',\n",
       " 'grapes',\n",
       " 'corn',\n",
       " 'meat',\n",
       " 'nati',\n",
       " 'Ethiopia',\n",
       " 'Whoso',\n",
       " 'Eliezer',\n",
       " 'heat',\n",
       " 'met',\n",
       " 'Naphtali',\n",
       " 'tongues',\n",
       " 'hith',\n",
       " 'drinketh',\n",
       " 'sac',\n",
       " 'occupation',\n",
       " 'Gomer',\n",
       " 'replenish',\n",
       " 'dwelling',\n",
       " 'grieved',\n",
       " 'fellow',\n",
       " 'now',\n",
       " 'lamp',\n",
       " 'rightly',\n",
       " 'deed',\n",
       " 'enough',\n",
       " 'overseer',\n",
       " 'live',\n",
       " 'eventide',\n",
       " 'seek',\n",
       " 'bad',\n",
       " 'Girgasite',\n",
       " 'ki',\n",
       " 'not',\n",
       " 'art',\n",
       " 'peop',\n",
       " 'tar',\n",
       " 'bak',\n",
       " 'mercies',\n",
       " 'shoulders',\n",
       " 'loved',\n",
       " 'pitcher',\n",
       " 'dust',\n",
       " 'let',\n",
       " 'sewed',\n",
       " 'sakes',\n",
       " 'al',\n",
       " 'whether',\n",
       " 'Potiphar',\n",
       " 'among',\n",
       " 'ghost',\n",
       " 'merchantmen',\n",
       " 'above',\n",
       " 'Rachel',\n",
       " 's',\n",
       " 'traffick',\n",
       " 'dine',\n",
       " 'embalm',\n",
       " 'priests',\n",
       " 'harvest',\n",
       " 'gold',\n",
       " 'fifth',\n",
       " 'Peradventure',\n",
       " 'according',\n",
       " 'too',\n",
       " 'law',\n",
       " 'them',\n",
       " 'bound',\n",
       " 'rise',\n",
       " 'reviv',\n",
       " 'fema',\n",
       " 'windows',\n",
       " 'lifted',\n",
       " 'stay',\n",
       " 'Beerlahairoi',\n",
       " 'Whose',\n",
       " 'fine',\n",
       " 'forgotten',\n",
       " 'bondman',\n",
       " 'suck',\n",
       " 'Teman',\n",
       " 'Jimnah',\n",
       " 'increase',\n",
       " 'Eshban',\n",
       " 'doubled',\n",
       " 'departing',\n",
       " 'divineth',\n",
       " 'stretched',\n",
       " 'Potipherah',\n",
       " 'hired',\n",
       " 'Moabites',\n",
       " 'put',\n",
       " 'Hanoch',\n",
       " 'Egyptia',\n",
       " 'Shuah',\n",
       " 'audience',\n",
       " 'waxen',\n",
       " 'renown',\n",
       " 'Methuselah',\n",
       " 'Lo',\n",
       " 'Amal',\n",
       " 'desolate',\n",
       " 'Uz',\n",
       " 'death',\n",
       " 'virgin',\n",
       " 'sinew',\n",
       " 'Beriah',\n",
       " 'Who',\n",
       " 'sheweth',\n",
       " 'openly',\n",
       " 'citi',\n",
       " 'sackcloth',\n",
       " 'li',\n",
       " 'sweet',\n",
       " 'dressed',\n",
       " 'shoulder',\n",
       " 'Ard',\n",
       " 'is',\n",
       " 'blessi',\n",
       " 'betimes',\n",
       " 'Earth',\n",
       " 'while',\n",
       " 'fo',\n",
       " 'exceedingly',\n",
       " 'fou',\n",
       " 'awoke',\n",
       " 'Peleg',\n",
       " 'fruitful',\n",
       " 'faces',\n",
       " 'Arkite',\n",
       " 'Slay',\n",
       " 'Beersheba',\n",
       " 'budded',\n",
       " 'Spirit',\n",
       " 'whomsoever',\n",
       " 'plenteous',\n",
       " 'you',\n",
       " 'For',\n",
       " 'royal',\n",
       " 'west',\n",
       " 'noon',\n",
       " 'follow',\n",
       " 'birthright',\n",
       " 'trained',\n",
       " 'Din',\n",
       " 'husband',\n",
       " 'second',\n",
       " 'roof',\n",
       " 'Magdiel',\n",
       " 'Why',\n",
       " 'Lamech',\n",
       " 'Gihon',\n",
       " 'Sheba',\n",
       " 'mocking',\n",
       " 'Shinar',\n",
       " 'observed',\n",
       " 'bade',\n",
       " 'ride',\n",
       " 'Manass',\n",
       " 'living',\n",
       " 'Buz',\n",
       " 'evening',\n",
       " 'pitched',\n",
       " 'Goshen',\n",
       " 'Ammon',\n",
       " 'weaned',\n",
       " 'En',\n",
       " 'secretly',\n",
       " 'Shalt',\n",
       " 'lower',\n",
       " 'folly',\n",
       " 'Let',\n",
       " 'pea',\n",
       " 'Edomites',\n",
       " 'words',\n",
       " 'life',\n",
       " 'wickedness',\n",
       " 'mouth',\n",
       " 'state',\n",
       " 'neither',\n",
       " 'Lay',\n",
       " 'Appoint',\n",
       " 'lambs',\n",
       " 'lodged',\n",
       " 'Pharaoh',\n",
       " 'laughed',\n",
       " 'Abidah',\n",
       " 'spi',\n",
       " 'nuts',\n",
       " 'serpent',\n",
       " 'sore',\n",
       " 'Job',\n",
       " 'frost',\n",
       " 'colts',\n",
       " 'Benam',\n",
       " 'couch',\n",
       " 'thistles',\n",
       " 'cunning',\n",
       " 'fat',\n",
       " 'Abraham',\n",
       " 'It',\n",
       " 'generation',\n",
       " 'wall',\n",
       " 'separated',\n",
       " 'curse',\n",
       " 'balm',\n",
       " 'seeth',\n",
       " 'through',\n",
       " 'feel',\n",
       " 'months',\n",
       " 'why',\n",
       " 'master',\n",
       " 'Seba',\n",
       " 'midwife',\n",
       " 'Bless',\n",
       " 'Manasseh',\n",
       " 'wa',\n",
       " 'or',\n",
       " 'weapons',\n",
       " 'w',\n",
       " 'done',\n",
       " 'Jamin',\n",
       " 'drank',\n",
       " 'bruise',\n",
       " 'me',\n",
       " 'secret',\n",
       " 'maiden',\n",
       " 'alo',\n",
       " '(',\n",
       " 'sworn',\n",
       " 'ha',\n",
       " 'hunt',\n",
       " 'youngest',\n",
       " 'strong',\n",
       " 'finding',\n",
       " 'Padanaram',\n",
       " 'Zeboiim',\n",
       " 'leaped',\n",
       " 'Isaac',\n",
       " 'force',\n",
       " 'embraced',\n",
       " 'Dinah',\n",
       " 'ward',\n",
       " 'it',\n",
       " 'fall',\n",
       " 'tempt',\n",
       " 'inn',\n",
       " 'keeper',\n",
       " 'Calneh',\n",
       " 'Tarshish',\n",
       " 'laugh',\n",
       " 'shepherd',\n",
       " 'deliver',\n",
       " 'Magog',\n",
       " 'dwell',\n",
       " 'calf',\n",
       " 'gracious',\n",
       " 'Shaul',\n",
       " 'fierce',\n",
       " 'Assyria',\n",
       " 'custom',\n",
       " 'supplanted',\n",
       " 'stole',\n",
       " 'fathers',\n",
       " 'Tubal',\n",
       " 'parcel',\n",
       " 'needs',\n",
       " 'travailed',\n",
       " 'sleep',\n",
       " 'whither',\n",
       " 'our',\n",
       " 'Gaham',\n",
       " 'Accad',\n",
       " 'because',\n",
       " 'herein',\n",
       " 'Sidon',\n",
       " 'hearth',\n",
       " 'angels',\n",
       " 'Arphaxad',\n",
       " 'formed',\n",
       " 'Rebekah',\n",
       " 'deep',\n",
       " 'fig',\n",
       " 'hindermost',\n",
       " 'created',\n",
       " 'roll',\n",
       " 'appease',\n",
       " 'verily',\n",
       " 'save',\n",
       " 'wat',\n",
       " 'break',\n",
       " 'betwixt',\n",
       " 'captain',\n",
       " 'tribute',\n",
       " 'Sodom',\n",
       " 'fugitive',\n",
       " 'straitly',\n",
       " 'mourn',\n",
       " 'thence',\n",
       " 'last',\n",
       " 'lovest',\n",
       " 'righteousness',\n",
       " 'married',\n",
       " 'This',\n",
       " 'Kirjatharba',\n",
       " 'deeds',\n",
       " 'draw',\n",
       " 'seventeen',\n",
       " 'Machpelah',\n",
       " 'Abrah',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By wrapping sorted() around the Python expression set(text3) [1], we obtain a sorted list of vocabulary items \n",
    "# beginning with various punctuation symbols and continuing with words starting with A. \n",
    "# All capitalized words precede lowercase words.\n",
    "set(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique words including punctuation marks\n",
    "len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct words is 6.230453042623537 % of the total number of words\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of distinct words is\",(len(set(text3)) / len(text3))*100,\"% of the total number of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awaken\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(text4[173])\n",
    "print(text4.index('awaken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'the', 'beginning']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m:n means elements mâ€¦n-1. We can omit the first number if the slice begins at the start of the list\n",
    "# we can omit the second number if the slice goes to the end:\n",
    "sent3[:3] \n",
    "# text2[141525:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said', 'than']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
    "           'more', 'is', 'said', 'than', 'done']\n",
    "tokens = set(saying)\n",
    "tokens = sorted(tokens)\n",
    "tokens[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 19317 samples and 260819 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = FreqDist(text1)\n",
    "print(fdist1)\n",
    "fdist1.N()\n",
    "#fdist1.most_common(20)\n",
    "\n",
    "#fdist1.tabulate() # Tabulate the frequency distribution\n",
    "\n",
    "#fdist1.plot(cumulative=True) # Cumulative plot of the frequency distribution\n",
    "\n",
    "#fdist1.plot() # Graphical plot of the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIRCUMNAVIGATION',\n",
       " 'Physiognomically',\n",
       " 'apprehensiveness',\n",
       " 'cannibalistically',\n",
       " 'characteristically',\n",
       " 'circumnavigating',\n",
       " 'circumnavigation',\n",
       " 'circumnavigations',\n",
       " 'comprehensiveness',\n",
       " 'hermaphroditical',\n",
       " 'indiscriminately',\n",
       " 'indispensableness',\n",
       " 'irresistibleness',\n",
       " 'physiognomically',\n",
       " 'preternaturalness',\n",
       " 'responsibilities',\n",
       " 'simultaneousness',\n",
       " 'subterraneousness',\n",
       " 'supernaturalness',\n",
       " 'superstitiousness',\n",
       " 'uncomfortableness',\n",
       " 'uncompromisedness',\n",
       " 'undiscriminating',\n",
       " 'uninterpenetratingly']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#14-19teens',\n",
       " '#talkcity_adults',\n",
       " '((((((((((',\n",
       " '........',\n",
       " 'Question',\n",
       " 'actually',\n",
       " 'anything',\n",
       " 'computer',\n",
       " 'cute.-ass',\n",
       " 'everyone',\n",
       " 'football',\n",
       " 'innocent',\n",
       " 'listening',\n",
       " 'remember',\n",
       " 'seriously',\n",
       " 'something',\n",
       " 'together',\n",
       " 'tomorrow',\n",
       " 'watching']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist5 = FreqDist(text5)\n",
    "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A collocation is a sequence of words that occur together unusually often. \n",
    "# Thus red wine is a collocation, whereas the wine is not.\n",
    "\n",
    "# To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as bigrams. \n",
    "# This is easily accomplished with the function bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; four years; years ago; Federal\n",
      "Government; General Government; American people; Vice President; Old\n",
      "World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;\n",
      "God bless; every citizen; Indian tribes; public debt; one another;\n",
      "foreign nations; political parties\n",
      "would like; medium build; social drinker; quiet nights; non smoker;\n",
      "long term; age open; Would like; easy going; financially secure; fun\n",
      "times; similar interests; Age open; weekends away; poss rship; well\n",
      "presented; never married; single mum; permanent relationship; slim\n",
      "build\n"
     ]
    }
   ],
   "source": [
    "text4.collocations()\n",
    "text8.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 19 samples and 260819 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({1: 47933,\n",
       "          2: 38513,\n",
       "          3: 50223,\n",
       "          4: 42345,\n",
       "          5: 26597,\n",
       "          6: 17111,\n",
       "          7: 14399,\n",
       "          8: 9966,\n",
       "          9: 6428,\n",
       "          10: 3528,\n",
       "          11: 1873,\n",
       "          12: 1053,\n",
       "          13: 567,\n",
       "          14: 177,\n",
       "          15: 70,\n",
       "          16: 22,\n",
       "          17: 12,\n",
       "          18: 1,\n",
       "          20: 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(w) for w in text1] #List of the lengths of words in text1\n",
    "fdist = FreqDist(len(w) for w in text1)\n",
    "print(fdist)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19255882431878046"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fdist.most_common()\n",
    "#fdist.max()\n",
    "#fdist[3]\n",
    "fdist.freq(3) # Percentage of words with length 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London', 'Park', 'Saffron']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(w for w in set(text1) if w.endswith('ableness'))\n",
    "\n",
    "# sorted(term for term in set(text4) if 'gnt' in term)\n",
    "\n",
    "# sorted(item for item in set(text6) if item.istitle())\n",
    "\n",
    "# sorted(item for item in set(sent7) if item.isdigit())\n",
    "\n",
    "# sorted(item for item in set(sent9) if item.isupper())\n",
    "\n",
    "sorted(item for item in set(sent9) if item.istitle())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17231"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)\n",
    "len(set(text1))\n",
    "len(set(word.lower() for word in text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16948"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word.lower() for word in text1 if word.isalpha())) #Converts to lower case alphabets\n",
    "#Eliminate numbers and punctuation from the vocabulary count by filtering out any non-alphabetic items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball, Lancaster, RegEx Based Stemmer\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preambl',\n",
       " 'wherea',\n",
       " 'recognit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inher',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalien',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr[:20]] # Still Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization less accurate but faste, vice versa for Stemming\n",
    "\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in udhr[:20]]\n",
    "\n",
    "#wn = nltk.WordNetLemmatizer()\n",
    "# print (wn.lemmatize(text))\n",
    "# print( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Data in pandas using Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday: The doctor's appointment is at 2:45pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday: The dentist's appointment is at 11:30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday: Be back home by 11:15 pm at the latest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0     Monday: The doctor's appointment is at 2:45pm.\n",
       "1  Tuesday: The dentist's appointment is at 11:30...\n",
       "2  Wednesday: At 7:00pm, there is a basketball game!\n",
       "3  Thursday: Be back home by 11:15 pm at the latest.\n",
       "4  Friday: Take the train at 08:10 am, arrive at ..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_sentences = [\"Monday: The doctor's appointment is at 2:45pm.\", \n",
    "                  \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "                  \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "                  \"Thursday: Be back home by 11:15 pm at the latest.\",\n",
    "                  \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    46\n",
       "1    50\n",
       "2    49\n",
       "3    49\n",
       "4    54\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of characters for each string in df['text']\n",
    "df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1     8\n",
       "2     8\n",
       "3    10\n",
       "4    10\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the number of tokens for each string in df['text']\n",
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: text, dtype: bool"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find which entries contain the word 'appointment'\n",
    "df['text'].str.contains('appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    3\n",
       "3    4\n",
       "4    8\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find how many times a digit occurs in each string\n",
    "df['text'].str.count(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [onday, he, doctor, s, appointment, is, at, 2,...\n",
       "1    [uesday, he, dentist, s, appointment, is, at, ...\n",
       "2    [ednesday, t, 7, 00pm, there, is, a, basketbal...\n",
       "3    [hursday, e, back, home, by, 11, 15, pm, at, t...\n",
       "4    [riday, ake, the, train, at, 08, 10, am, arriv...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all occurances of the digits\n",
    "#df['text'].str.findall(r'\\d')\n",
    "#df['text'].str.findall(\"[a-z]+\")\n",
    "df['text'].str.findall(\"[a-z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [(2, 45)]\n",
       "1              [(11, 30)]\n",
       "2               [(7, 00)]\n",
       "3              [(11, 15)]\n",
       "4    [(08, 10), (09, 00)]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group and find the hours and minutes\n",
    "df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          ???: The doctor's appointment is at 2:45pm.\n",
       "1       ???: The dentist's appointment is at 11:30 am.\n",
       "2          ???: At 7:00pm, there is a basketball game!\n",
       "3         ???: Be back home by 11:15 pm at the latest.\n",
       "4    ???: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace weekdays with '???'\n",
    "df['text'].str.replace(r'\\w+day\\b', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Mon: The doctor's appointment is at 2:45pm.\n",
       "1       Tue: The dentist's appointment is at 11:30 am.\n",
       "2          Wed: At 7:00pm, there is a basketball game!\n",
       "3         Thu: Be back home by 11:15 pm at the latest.\n",
       "4    Fri: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace weekdays with 3 letter abbrevations\n",
    "df['text'].str.replace(r'(\\w+day\\b)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0   2  45\n",
       "1  11  30\n",
       "2   7  00\n",
       "3  11  15\n",
       "4  08  10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new columns from first match of extracted groups\n",
    "df['text'].str.extract(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11:30 am</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08:10 am</td>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0   1   2   3\n",
       "  match                      \n",
       "0 0        2:45pm   2  45  pm\n",
       "1 0      11:30 am  11  30  am\n",
       "2 0        7:00pm   7  00  pm\n",
       "3 0      11:15 pm  11  15  pm\n",
       "4 0      08:10 am  08  10  am\n",
       "  1       09:00am  09  00  am"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the entire time, the hours, the minutes, and the period\n",
    "df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11:30 am</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08:10 am</td>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             time hour minute period\n",
       "  match                             \n",
       "0 0        2:45pm    2     45     pm\n",
       "1 0      11:30 am   11     30     am\n",
       "2 0        7:00pm    7     00     pm\n",
       "3 0      11:15 pm   11     15     pm\n",
       "4 0      08:10 am   08     10     am\n",
       "  1       09:00am   09     00     am"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the entire time, the hours, the minutes, and the period with group names\n",
    "df['text'].str.extractall(r'(?P<time>(?P<hour>\\d?\\d):(?P<minute>\\d\\d) ?(?P<period>[ap]m))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
